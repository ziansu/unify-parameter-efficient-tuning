import torch
import torch.nn as nn
import torch.nn.functional as F
import math


def init_lisa_params(module):
    std = 1e-20
    if isinstance(module, nn.Linear):
        module.weight.data.normal_(mean=0.0, std=std)
        if module.bias is not None:
            module.bias.data.zero_()
    elif isinstance(module, nn.Embedding):
        module.weight.data.normal_(mean=0.0, std=std)
        if module.padding_idx is not None:
            module.weight.data[module.padding_idx].zero_()


def init_bias_mlp(module):
    std = 1e-2
    if isinstance(module, nn.Linear):
        module.weight.data.normal_(mean=0.0, std=std)
        if module.bias is not None:
            module.bias.data.zero_()


def init_bert_weights(module):
    """Initialize the weights."""
    if isinstance(module, (nn.Linear, nn.Embedding)):
        # std defaults to 0.02, this might need to be changed
        module.weight.data.normal_(mean=0.0, std=0.02)
    elif isinstance(module, nn.LayerNorm):
        module.bias.data.zero_()
        module.weight.data.fill_(1.0)
    if isinstance(module, nn.Linear) and module.bias is not None:
        module.bias.data.zero_()


def init_zero_weights(module):
    if isinstance(module, nn.Embedding):
        nn.init.constant_(module.weight, 0.0)


# def init_hyper(weight, bias):
# NOTE: is equivalent to init_bert_weights


# prototype, deprecated, unable to parallel
class Dep1LightMixAdapter_Layer(nn.Module):
    """
    FIXME: seems not working with nn.DataParallel

    Generated by Hypernetworks (controller)
    """

    def __init__(self,
                 d_model=None,
                 bottleneck=None,
                 dropout=0.0,
                 init_option="bert",
                 adapter_scalar="1.0",
                 adapter_layernorm_option="in",
                 n_types=None,
                 hypernet_output=None):
        super().__init__()
        assert hypernet_output

        self.n_embd = d_model
        self.bottleneck = bottleneck
        self.n_types = n_types

        #_before
        self.adapter_layernorm_option = adapter_layernorm_option

        self.adapter_layer_norm_before = None
        if adapter_layernorm_option == "in" or adapter_layernorm_option == "out":
            self.adapter_layer_norm_before = nn.LayerNorm(self.n_embd)

        if adapter_scalar == "learnable_scalar":
            self.scale = nn.Parameter(torch.ones(1))
        else:
            self.scale = float(adapter_scalar)
        
        self.non_linear_func = nn.ReLU()
        self.dropout = dropout

        # NOTE: initialization is done by controller
        if init_option == "bert":
            # self.apply(init_bert_weights)
            pass
        elif init_option == "lora":
            # with torch.no_grad():
            #     nn.init.kaiming_uniform_(self.down_proj.weight, a=math.sqrt(5))
            #     nn.init.zeros_(self.up_proj.weight)
            #     nn.init.zeros_(self.down_proj.bias)
            #     nn.init.zeros_(self.up_proj.bias)
            # raise NotImplementedError
            pass

        # NOTE: currently global same as local

        # [Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html?highlight=linear#torch.nn.Linear)
        # weight: [bottleneck x n_embd] -> (n_types x bottleneck) x n_embd
        # bias: [bottleneck] -> n_types x bottleneck
        self.down_weight = [d[0] for d in hypernet_output['down']]
        self.down_bias = [d[1] for d in hypernet_output['down']]
        self.down_weight = torch.cat(self.down_weight, dim=0)
        assert self.down_weight.shape[1] == self.n_embd
        self.down_bias = torch.cat(self.down_bias)

        # [Conv1d](https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#torch.nn.Conv1d)
        # in_channels / groups = bottleneck
        # out_channels = n_embd x n_types
        # kernel_size = 1
        # weight: [n_embd x bottleneck] -> (n_types x n_embd) x bottleneck x 1
        # bias: [n_embd] -> n_types x n_embd
        self.up_weight = [u[0] for u in hypernet_output['up']]
        self.up_bias = [u[1] for u in hypernet_output['up']]
        self.up_weight = torch.cat(self.up_weight, dim=0).view(n_types * self.n_embd, self.bottleneck, 1)
        self.up_bias = torch.cat(self.up_bias)

    def forward(self, x, anno, add_residual=True, residual=None):

        residual = x if residual is None else residual
        if self.adapter_layernorm_option == 'in':
            x = self.adapter_layer_norm_before(x)
        
        # parallel down & up projection
        down = F.linear(x, weight=self.down_weight, bias=self.down_bias)
        down = self.non_linear_func(down)
        down = nn.functional.dropout(down, p=self.dropout, training=self.training)
        up = F.conv1d(down.permute(0, 2, 1), weight=self.up_weight, bias=self.up_bias, groups=self.n_types)
        up = up.permute(0, 2, 1).view(x.shape[0], x.shape[1], self.n_types, self.n_embd)
        # print('up shape:', up.shape)

        # mask out unrelated parts
        for i in range(1, self.n_types):  # first global
            up[:,:,i,:] = up[:,:,i,:].masked_fill_((anno!=i)[:,:,None].expand(x.size()), 0.0)

        up = torch.sum(up, dim=2)
        up = up * self.scale

        if self.adapter_layernorm_option == 'out':
            up = self.adapter_layer_norm_before(up)

        if add_residual:
            output = up + residual
        else:
            output = up

        return output


class LightMixAdapter_Layer(nn.Module):
    """
    Mimic original hyper adapter

    Maybe allow independent global 
    """

    def __init__(self,
                 d_model=None,
                 bottleneck=None,
                 dropout=0.0,
                 init_option="bert",
                 adapter_scalar="1.0",
                 adapter_layernorm_option="in",
                 n_types=None,
                 layer_id=None):
        """
        hypernet is to generate weight and bias everytime we use this adapter
        less efficient but enables easy data parallel
        """
        super().__init__()

        self.n_embd = d_model
        self.bottleneck = bottleneck
        self.n_types = n_types
        self.layer_id = layer_id

        #_before
        self.adapter_layernorm_option = adapter_layernorm_option

        self.adapter_layer_norm_before = None
        if adapter_layernorm_option == "in" or adapter_layernorm_option == "out":
            self.adapter_layer_norm_before = nn.LayerNorm(self.n_embd)

        if adapter_scalar == "learnable_scalar":
            self.scale = nn.Parameter(torch.ones(1))
        else:
            self.scale = float(adapter_scalar)
        
        self.non_linear_func = nn.ReLU()
        self.dropout = dropout

        # NOTE: initialization is done by controller
        if init_option == "bert":
            # self.apply(init_bert_weights)
            pass
        elif init_option == "lora":
            # with torch.no_grad():
            #     nn.init.kaiming_uniform_(self.down_proj.weight, a=math.sqrt(5))
            #     nn.init.zeros_(self.up_proj.weight)
            #     nn.init.zeros_(self.down_proj.bias)
            #     nn.init.zeros_(self.up_proj.bias)
            # raise NotImplementedError
            pass

        # NOTE: currently global same as local


    def forward(self, x, anno, hypernet=None, add_residual=True, residual=None):
        assert hypernet

        # prepare weights
        down_list, up_list = hypernet.pack_all_weight_and_bias(layer_id=self.layer_id, transpose=True)

        # [Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html?highlight=linear#torch.nn.Linear)
        # weight: [bottleneck x n_embd] -> (n_types x bottleneck) x n_embd
        # bias: [bottleneck] -> n_types x bottleneck
        self.down_weight = [d[0] for d in down_list]
        self.down_bias = [d[1] for d in down_list]
        self.down_weight = torch.cat(self.down_weight, dim=0)
        assert self.down_weight.shape[1] == self.n_embd
        self.down_bias = torch.cat(self.down_bias)

        # [Conv1d](https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#torch.nn.Conv1d)
        # in_channels / groups = bottleneck
        # out_channels = n_embd x n_types
        # kernel_size = 1
        # weight: [n_embd x bottleneck] -> (n_types x n_embd) x bottleneck x 1
        # bias: [n_embd] -> n_types x n_embd
        self.up_weight = [u[0] for u in up_list]
        self.up_bias = [u[1] for u in up_list]
        self.up_weight = torch.cat(self.up_weight, dim=0).view(self.n_types * self.n_embd, self.bottleneck, 1)
        self.up_bias = torch.cat(self.up_bias)


        residual = x if residual is None else residual
        if self.adapter_layernorm_option == 'in':
            x = self.adapter_layer_norm_before(x)
        
        # parallel down & up projection
        down = F.linear(x, weight=self.down_weight, bias=self.down_bias)
        down = self.non_linear_func(down)
        down = nn.functional.dropout(down, p=self.dropout, training=self.training)
        up = F.conv1d(down.permute(0, 2, 1), weight=self.up_weight, bias=self.up_bias, groups=self.n_types)
        up = up.permute(0, 2, 1).view(x.shape[0], x.shape[1], self.n_types, self.n_embd)
        # print('up shape:', up.shape)

        # mask out unrelated parts
        for i in range(1, self.n_types):  # first global
            up[:,:,i,:] = up[:,:,i,:].masked_fill_((anno!=i)[:,:,None].expand(x.size()), 0.0)

        up = torch.sum(up, dim=2)
        up = up * self.scale

        if self.adapter_layernorm_option == 'out':
            up = self.adapter_layer_norm_before(up)

        if add_residual:
            output = up + residual
        else:
            output = up

        return output
        

class LayerAnnoLinearHyperNet(nn.Module):
    "For up / down projection"

    def __init__(self, config, input_dim, output_dim, init_option='bert'):
        super(LayerAnnoLinearHyperNet, self).__init__()
        # 2 * d_model * d_bn
        self.hyper_bn = config.hyper_bn # d_p
        self.weight_generator = nn.Linear(self.hyper_bn, input_dim*output_dim)
        self.bias_generator = nn.Linear(self.hyper_bn, output_dim)  # FIXME: original version is `input_dim`, which I think incorrect

        self.input_dim = input_dim
        self.output_dim = output_dim

        # TODO: figure out proper initialization
        if init_option == 'bert':
            init_bert_weights(self.weight_generator)
        elif init_option == 'lora':
            nn.init.kaiming_uniform_(self.weight_generator.weight, a=math.sqrt(5))
            nn.init.zeros_(self.weight_generator.bias)
        elif init_option == 'zero':
            nn.init.zeros_(self.weight_generator.weight)
            nn.init.zeros_(self.weight_generator.bias)
        nn.init.zeros_(self.bias_generator.weight)
        nn.init.zeros_(self.bias_generator.bias)

    def forward(self, hyper_proj_embd, transpose=False):
        hyper_proj_embd = hyper_proj_embd.view(-1)
        if transpose:   # no need to actually transpose because of random initialization
            weight = self.weight_generator(hyper_proj_embd).view(self.output_dim, self.input_dim)
        else:
            weight = self.weight_generator(hyper_proj_embd).view(self.input_dim, self.output_dim)
        bias = self.bias_generator(hyper_proj_embd).view(-1)
        return weight, bias


class LayerAnnoLNHyperNet(nn.Module):
    """This module generates the weight and bias for the task conditioned layer norm."""

    def __init__(self, config, input_dim):
        super(LayerAnnoLNHyperNet, self).__init__()
        self.input_dim = input_dim
        self.hyper_bn = config.hyper_bn
        self.weight_generator = nn.Linear(self.hyper_bn, self.input_dim, bias=False)
        self.bias_generator = nn.Linear(self.hyper_bn, self.input_dim, bias=False)

    def forward(self, input):
        return self.weight_generator(input), self.bias_generator(input)


class LayerAnnoController(nn.Module):
    "Mimic `class AdapterLayersOneHyperNetController`"
    # TODO: consider adapter position id (personall I don't think this is necessary)

    def __init__(self, 
                config,
                n_layers,
                local_bn,
                init_option,
                adapter_scalar,
                adapter_layernorm_option='none',
                ):
        super(LayerAnnoController, self).__init__()

        if hasattr(config, 'n_embd'):
            self.n_embd = config.n_embd
        elif hasattr(config, 'd_model'):
            self.n_embd = config.d_model
        elif hasattr(config, 'hidden_size'):
            self.n_embd = config.hidden_size
        else:
            raise NotImplementedError

        self.device = config.device
        self.n_layers = n_layers
        self.local_bn = local_bn    # rename this
        self.init_option = init_option
        self.adapter_scalar = adapter_scalar
        self.adapter_layernorm_option = adapter_layernorm_option

        self.n_types = config.lp_num + 1    # include global because global start from 0 (also consider including global into hypernet)
        self.hyper_embd_dim = config.hyper_embd_dim
        self.hyper_proj_intermediate_dim = config.hyper_proj_intermediate_dim
        self.hyper_bn = config.hyper_bn     # projection dim, d_p

        # embeddings
        self.layer_embedding = nn.Embedding(self.n_layers, self.hyper_embd_dim)
        self.type_embedding = nn.Embedding(self.n_types, self.hyper_embd_dim)

        # projection MLP
        self.embd_proj = nn.Sequential(
            nn.Linear(2 * self.hyper_embd_dim, self.hyper_proj_intermediate_dim),
            nn.ReLU(),
            nn.Linear(self.hyper_proj_intermediate_dim, self.hyper_bn)
        )

        # weight & bias hypernet
        self.down_proj_hypernet = LayerAnnoLinearHyperNet(config, self.n_embd, self.local_bn, init_option='lora')
        self.up_proj_hypernet = LayerAnnoLinearHyperNet(config, self.local_bn, self.n_embd, init_option='zero')

        # layernorm hypernet
        if self.adapter_layernorm_option != 'none':
            self.ln_hypernet = LayerAnnoLNHyperNet(config, self.n_embd)

    def get_embedding(self, layer_id, type_id):
        layer_embd = self.layer_embedding(layer_id)
        type_embd = self.type_embedding(type_id)
        concat_embd = torch.cat((layer_embd, type_embd), dim=-1)
        proj_embd = self.embd_proj(concat_embd)
        return proj_embd
    
    def forward(self, layer_id, anno_id):
        proj_embd = self.get_embedding(layer_id, anno_id)
        down = self.down_proj_hypernet(proj_embd)
        up = self.up_proj_hypernet(proj_embd)
        return down, up

    def pack_all_weight_and_bias(self, layer_id, transpose=False):
        """
        generate all the weight matrices of some local adapter

        NOTE: global included version
        """
        layer_id = torch.tensor([layer_id], dtype=torch.long, device=self.device)
        proj_embd_list = [self.get_embedding(layer_id, torch.tensor([anno_id], dtype=torch.long, device=self.device)) for anno_id in range(self.n_types)]
        down_list = [self.down_proj_hypernet(pe, transpose) for pe in proj_embd_list]
        up_list = [self.up_proj_hypernet(pe, transpose) for pe in proj_embd_list]
        return down_list, up_list

    def pack_mix_adapter(self, layer_id):
        "Deprecated"
        down_list, up_list = self.pack_all_weight_and_bias(layer_id=layer_id, transpose=True)
        adapter = LightMixAdapter_Layer(
            d_model=self.n_embd,
            bottleneck=self.local_bn,
            dropout=0.0,
            init_option=self.init_option,
            adapter_scalar=self.adapter_scalar,
            adapter_layernorm_option=self.adapter_layernorm_option,
            n_types=self.n_types,
            hypernet_output={'down': down_list, 'up': up_list}
        )
        return adapter
